# -*- coding: utf-8 -*-
"""EMPLOYEE SALARY PREDICTION (4).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rwtIXo4bseUf5Qf-xC6VDd9zKmeDeOtj
"""

#Employee salary prediction using adult csv
#load ur libraries

!pip install pandas

!pip install seaborn

import pandas as pd

"""from google.colab import drive
drive.mount('/content/drive')



"""

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/adult 3.csv'
data = pd.read_csv(file_path)
print("‚úÖ Dataset loaded successfully!")

data

data.shape
#rows and columns shows

data.head()
#upr se dikhayega

data.head(7)

data.tail()
# niche se dikhayega

data.tail(7)

#findinf null values
#isna= is there any null value
data.isna()
#if true then null value is there, if false, then no null value

data.isna().sum()
#retrive all the null values under each column
#if u dont have any null values u r good to go wit the dataset

#lets take the first variable i.e workclass
#we will undrstnd, the value under each column is category values or any other value
#we will use the method called value count
#categorical data
print(data.occupation.value_counts())
#the result shows which one is dominating in the occupation section
#there is a questionmark, what we need to do is remove the special character with value

print(data.gender.value_counts())

print(data.education.value_counts())

print(data.workclass.value_counts())
#without pay or never worked wont actually contribute to salary prediction

#remove these two data..we consider it as dimenionality reduction
#to access the workclass, we we consider data of workclass

data=data[data['workclass']!='Without-pay']
data=data[data['workclass']!='Never-worked']

print(data.workclass.value_counts())

print(data['marital-status'].value_counts())

print(data['age'].value_counts())
#80+ is not contributing, so we can remove them to speed up the process

#there is a questionmark, what we need to do is remove the special character with value
#we will replace it will others category
#Replacing the name of the category with some other suitable name

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Replace with your Drive path
file_path = '/content/drive/My Drive/adult 3.csv'
data = pd.read_csv(file_path)

# Replace '?' with 'NA'
data['occupation'] = data['occupation'].replace({'?': 'NA'})

# Encode occupation names
encoder = LabelEncoder()
data['occupation_encoded'] = encoder.fit_transform(data['occupation'])

# Decode to check
data['occupation_decoded'] = encoder.inverse_transform(data['occupation_encoded'])

# Display results
print(data[['occupation', 'occupation_encoded', 'occupation_decoded']].head())

print(data['occupation'].value_counts())

data.shape

#REDUNDENCY
#TWO DIFF COLUMNS CONVEYING THE SAME INFORMATION
#here education and educational no. is doing the same
#we will remove education cz it is categorical value..nd we prefer numerical value
#drop is the function used to delete a particular column from the dataset

data.drop(columns=['education'],inplace=True)

data

#OUTLIER
import matplotlib.pyplot as plt
plt.boxplot(data['age'])
plt.show()
#this shows numerical values not the catagorical values
#from 80 to 90, the circle rep outliers
#20 to 30 freshers
#25 to 45 adult experienced employees
#45 to 60 senior employees
#most important employees : 25 to 45 the box represents

#considerable range
data=data[(data['age']<=75)&(data['age']>=17)]

plt.boxplot(data['age'])
plt.show()
#removing the outliers

# Commented out IPython magic to ensure Python compatibility.
# %pip install scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# %pip install scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# %pip install numpy scipy scikit-learn --force-reinstall

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load dataset
# Replace with your Drive path
file_path = '/content/drive/My Drive/adult 3.csv'
data = pd.read_csv(file_path)  # Replace with your file path

# Create LabelEncoder instance
#assign some unique no. to each category of the column
encoder = LabelEncoder()

# Encode categorical columns
data['workclass'] = encoder.fit_transform(data['workclass'])
data['marital-status'] = encoder.fit_transform(data['marital-status'])
data['occupation'] = encoder.fit_transform(data['occupation'])
data['relationship'] = encoder.fit_transform(data['relationship'])
data['race'] = encoder.fit_transform(data['race'])
data['gender'] = encoder.fit_transform(data['gender'])
data['native-country'] = encoder.fit_transform(data['native-country'])

# Display the encoded dataset
print(data.head())

#first we need to do splitting of data into training and testing

#Dividing the data set into two parts : Input and Output
 x = data.drop(columns=['income']) #input
 y = data['income'] #output

x

y

print(x.dtypes)

#education in in object type, but we need it in int64 to get the desired outcome
from sklearn.preprocessing import LabelEncoder

# Make a copy to avoid modifying the original
x_encoded = x.copy()

# Encode 'education' column
le = LabelEncoder()
x_encoded['education'] = le.fit_transform(x_encoded['education'])

# Now 'education' will be int64
print(x_encoded.dtypes)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
x = scaler.fit_transform(x_encoded)
x

# Back to DataFrame if needed
x_df = pd.DataFrame(x, columns=x_encoded.columns)

#for training and testing
from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size=0.2, random_state=43, stratify=y)
# x ones are input training data, we can give any no. for the random state, stratify will evenly distribute the values
# test_size is test data split up ratio, which is default ratio is 20 % thats why given 0.2
# random state ensures for any given condition the ratio should be 20-80
# xtrain and ytrain is used to make the machine learn patterns
# xtest and ytest is used to see if my model is working properly or not

xtrain
# successfully split up the data for training and testing

# creating a ml algorithm - supervised machine learning algorithm
from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(xtrain, ytrain) #input and output training data
predict=knn.predict(xtest)
predict

# after prediction it is the turn of result and evaluation
# ytest is the key to predict the answers

# 3 times with different way we will predict the accuracy

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict)

#logistic regretion
from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
lr.fit(xtrain, ytrain) #input and output training data
predict1=lr.predict(xtest)
predict1

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict1)

from sklearn.neural_network import MLPClassifier
clf = MLPClassifier(solver='adam', hidden_layer_sizes=(5,2), random_state=2, max_iter=2000)
clf.fit(xtrain, ytrain)
predict2=lr.predict(xtest)
predict2

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict2)

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, OneHotEncoder

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

models = {
    "LogisticRegression" : LogisticRegression(),
    "RandomForest" : RandomForestClassifier(),
    "KNN" : KNeighborsClassifier(),
    "SVM" : SVC(),
    "GradientBoosting" : GradientBoostingClassifier()
}

results = {}

for name, model in models.items():
    pipe = Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"{name} Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
plt.bar(results.keys(), results.values(), color='skyblue')
plt.ylabel('Accuracy Score')
plt.title('Model Comparison')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

import pandas as pd

file_path = '/content/drive/My Drive/adult 3.csv'
data = pd.read_csv(file_path)

# Show first few rows
print("‚úÖ Data loaded successfully:")
data.head()

data.replace("?", pd.NA, inplace=True)
data.dropna(inplace=True)  # or use fillna()

data_encoded = pd.get_dummies(data, drop_first=True)

X = data_encoded.drop("income_>50K", axis=1)  # target is income
y = data_encoded["income_>50K"]  # 1 if >50K, else 0

!pip install seaborn

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.boxplot(x='income', y='age', data=data)
plt.title("Salary vs Age")
plt.show()

plt.figure(figsize=(12,6))
sns.boxplot(x='education', y='hours-per-week', hue='income', data=data)
plt.xticks(rotation=45)
plt.title("Salary vs Education")
plt.show()

plt.figure(figsize=(8,5))
sns.barplot(x='gender', y='hours-per-week', hue='income', data=data)
plt.title("Gender Pay Gap Analysis")
plt.show()

top_countries = data['native-country'].value_counts().head(10).index
data_top_countries = data[data['native-country'].isin(top_countries)]

plt.figure(figsize=(12,6))
sns.countplot(y='native-country', hue='income', data=data_top_countries)
plt.title("Salary Distribution by Country")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Evaluate
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))

import numpy as np

# Get importances and features
importances = model.feature_importances_
features = X.columns

# Sort features and importances
sorted_idx = np.argsort(importances)[::-1]
sorted_features = features[sorted_idx]
sorted_importances = importances[sorted_idx]

import numpy as np
import matplotlib.pyplot as plt

# Sort features by importance
importances = model.feature_importances_
features = X.columns
sorted_idx = np.argsort(importances)[::-1]

# Plot Top 15 Features
top_n = 15
plt.figure(figsize=(10,6))
plt.barh(features[sorted_idx][:top_n][::-1], importances[sorted_idx][:top_n][::-1])
plt.title("Top 15 Feature Importances from RandomForest")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib

# Align y to x if needed
if len(y) > len(x):
    y = y[:len(x)]
elif len(x) > len(y):
    x = x[:len(y)]

# Confirm shapes
print("‚úÖ x shape:", x.shape)
print("‚úÖ y shape:", y.shape)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Define models
models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "RandomForest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "GradientBoosting": GradientBoostingClassifier()
}

results = {}

# Train and evaluate
for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)
    results[name] = acc
    print(f"{name}: {acc:.4f}")

# Get best model
best_model_name = max(results, key=results.get)
best_model = models[best_model_name]
print(f"\n‚úÖ Best model: {best_model_name} with accuracy {results[best_model_name]:.4f}")

# Save the best model
joblib.dump(best_model, "best_model.pkl")
print("‚úÖ Saved best model as best_model.pkl")

import matplotlib.pyplot as plt

# Plot model accuracy comparison
plt.figure(figsize=(8,6))
plt.bar(results.keys(), results.values(), color='skyblue')
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.xlabel("Models")
plt.ylim(0,1)
plt.grid(axis='y')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# 
# # Load the trained model
# model = joblib.load("best_model.pkl")
# 
# # Set up the Streamlit app
# st.set_page_config(page_title="WageWise: Smart Salary Analyzer", page_icon="üíº", layout="centered")
# 
# st.title("üíº WageWise: Smart Salary Analyzer")
# st.markdown("Predict how much an employee earns based on their details. Gain quick insights into what factors drive salaries in the organization.")
# 
# # Sidebar inputs
# st.sidebar.header("üìã Enter Employee Details")
# 
# # User input fields (adjusted for dataset columns)
# age = st.sidebar.slider("Age", 18, 65, 30)
# education = st.sidebar.selectbox("Education Level", [
#     "Bachelors", "Masters", "PhD", "HS-grad", "Assoc", "Some-college"
# ])
# occupation = st.sidebar.selectbox("Job Role", [
#     "Tech-support", "Craft-repair", "Other-service", "Sales",
#     "Exec-managerial", "Prof-specialty", "Handlers-cleaners", "Machine-op-inspct",
#     "Adm-clerical", "Farming-fishing", "Transport-moving", "Priv-house-serv",
#     "Protective-serv", "Armed-Forces"
# ])
# hours_per_week = st.sidebar.slider("Hours per week", 1, 80, 40)
# experience = st.sidebar.slider("Years of Experience", 0, 40, 5)
# 
# # Create DataFrame from inputs
# input_df = pd.DataFrame({
#     'age': [age],
#     'education': [education],
#     'occupation': [occupation],
#     'hours-per-week': [hours_per_week],
#     'experience': [experience]
# })
# 
# st.write("### üìù Input Summary")
# st.table(input_df)
# 
# # Predict button
# if st.button("üîÆ Predict Salary Class"):
#     prediction = model.predict(input_df)
#     probability = model.predict_proba(input_df).max() * 100 if hasattr(model, 'predict_proba') else None
# 
#     if prediction[0] == '>50K':
#         st.success(f"üéØ This employee is likely to earn **above $50K/year**.")
#     else:
#         st.warning(f"üí° This employee is likely to earn **$50K/year or less**.")
# 
#     if probability:
#         st.markdown(f"**Confidence Level:** {probability:.2f}%")
# 
#     st.balloons()
# 
# # Batch prediction section
# st.markdown("---")
# st.markdown("### üìÇ Batch Prediction for Multiple Employees")
# uploaded_file = st.file_uploader("Upload a CSV file for salary prediction", type="csv")
# 
# if uploaded_file is not None:
#     batch_data = pd.read_csv(uploaded_file)
#     st.write("üìÑ Uploaded Data Preview:")
#     st.dataframe(batch_data.head())
# 
#     batch_preds = model.predict(batch_data)
#     batch_data['PredictedClass'] = batch_preds
# 
#     # Show summary analytics
#     above_50k = (batch_preds == '>50K').sum()
#     total = len(batch_preds)
#     st.markdown(f"‚úÖ **{above_50k}/{total} employees** predicted to earn >50K.")
# 
#     st.write("üìä Predictions with added column:")
#     st.dataframe(batch_data.head())
# 
#     csv = batch_data.to_csv(index=False).encode('utf-8')
#     st.download_button("üì• Download Predictions CSV", csv, file_name='salary_predictions.csv', mime='text/csv')
# 
# st.markdown("---")
# st.markdown("üë©‚Äçüíª *Built with ‚ù§Ô∏è for HR professionals and data enthusiasts.*")
# 
# 
#

!pip install streamlit pyngrok

!ngrok authtoken 309KFcPtopLpMIdnfjFPw7XLBsW_67HT4VY7gGeeYZ6Yhd5nu

import os
import threading

def run_streamlit():
    os.system('streamlit run app.py --server.port 8501')

thread = threading.Thread(target = run_streamlit)
thread.start()

from pyngrok import ngrok
import time

#wait a few sec
time.sleep(5)

#create a tunnel to streamlit port 8501
public_url = ngrok.connect(8501)
print("Your Streamlit app is live here:", public_url)

!streamlit run app.py























